{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e690b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a random change\n",
    "\n",
    "#calculating distance: gridrefGDF['DistToRiver'] = gridrefGDF[\"geometry\"].apply(lambda x: df.distance(x).min())\n",
    "#season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500ceffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import os\n",
    "import glob\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import LineString, MultiPolygon\n",
    "from shapely.geometry import Point\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import math\n",
    "from math import cos, asin, sqrt\n",
    "import csv\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db63f13-57c8-4e58-9570-931b5fb51d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def import class\n",
    "def import_csv(input_no, folder_path, csv_files):\n",
    "    if input_no < 1 or input_no > len(csv_files):\n",
    "        raise ValueError(\"input_no should be between 1 and the number of CSV files\")\n",
    "    # Create the full path for each CSV file\n",
    "    full_paths = [os.path.join(folder_path, csv) for csv in csv_files[:input_no]]\n",
    "\n",
    "    # Read CSV files into DataFrames\n",
    "    dfs = [pd.read_csv(full_path, dtype={\"INTERPRETATION\": str, \"TEXT_RESULT\": str, \"DETCODE\": str, \"PTCODE\":str}) for full_path in full_paths]\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    full_wims = pd.concat(dfs)\n",
    "\n",
    "    return full_wims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f9c95-8ed4-4990-bc18-67acc3af601f",
   "metadata": {},
   "source": [
    "## Getting rid of any duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e24230b-5a14-4177-909d-daa557498e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rows(full_wims_all):\n",
    "    duplicate_rows = full_wims_all[full_wims_all.duplicated()]\n",
    "    if duplicate_rows.empty:\n",
    "        print(\"No duplicate rows found.\")\n",
    "        full_wims_no_dup = full_wims_all\n",
    "    else:\n",
    "        num_duplicates = len(duplicate_rows)\n",
    "        num_wims = len(full_wims_all)\n",
    "        print(f\"Number of duplicate rows found: {num_duplicates}\")\n",
    "        print(f\"Total number of rows in dataset: {num_wims}\")\n",
    "        full_wims_no_dup = full_wims_all.drop_duplicates()\n",
    "        num_no_dup = len(full_wims_no_dup)\n",
    "        print(f\"Total number of rows in dataset after duplicates removed: {num_no_dup}\")\n",
    "\n",
    "    return full_wims_no_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a83df67-d432-437f-9dde-7401f270b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_shapefiles(folder_path):\n",
    "    # Find all shapefiles in the specified folder\n",
    "    shapefile_paths = glob.glob(os.path.join(folder_path, '*.shp'))\n",
    "\n",
    "    if not shapefile_paths:\n",
    "        raise FileNotFoundError(\"No shapefiles found in the specified folder.\")\n",
    "\n",
    "    # If there is only one shapefile, read it directly\n",
    "    if len(shapefile_paths) == 1:\n",
    "        return gpd.read_file(shapefile_paths[0])\n",
    "\n",
    "    # If there are multiple shapefiles, read and union them\n",
    "    gdf_list = [gpd.read_file(shapefile) for shapefile in shapefile_paths]\n",
    "    combined_shapefile = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "\n",
    "    return combined_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cfc5d3-56f4-448b-b32d-20a4c6e6b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shapefile(data_dict, estuary_to_extract, input_shapefile, target_crs='EPSG:xxxx'):\n",
    "    if estuary_to_extract in data_dict:\n",
    "        # Get the corresponding values from the dictionary\n",
    "        corresponding_values = data_dict[estuary_to_extract]\n",
    "\n",
    "        # Initialize an empty GeoDataFrame\n",
    "        combined_gdf = gpd.GeoDataFrame()\n",
    "    \n",
    "        # Iterate through each corresponding value\n",
    "        for value in corresponding_values:\n",
    "            # Filter rows based on the current value\n",
    "            subset_gdf = input_shapefile[input_shapefile['wb_name'] == value]\n",
    "\n",
    "            # Check if the subset is not empty before combining\n",
    "            if not subset_gdf.empty:\n",
    "                # Concatenate the subset to the combined GeoDataFrame\n",
    "                combined_gdf = pd.concat([combined_gdf, subset_gdf], ignore_index=True)\n",
    "\n",
    "        # Check if there is more than one row for the key\n",
    "        if len(combined_gdf) > 1:\n",
    "            # Use unary_union to combine geometries\n",
    "            combined_geometry = unary_union(combined_gdf['geometry'])\n",
    "\n",
    "            # Create a new GeoDataFrame with the combined geometry\n",
    "            new_gdf = gpd.GeoDataFrame(geometry=[combined_geometry], crs=target_crs)\n",
    "\n",
    "            # Print or do further processing with the new GeoDataFrame\n",
    "            print(new_gdf)\n",
    "        elif not combined_gdf.empty:\n",
    "            # Only one row, no need to combine, use the original GeoDataFrame\n",
    "            new_gdf = combined_gdf.copy()\n",
    "            new_gdf.crs = target_crs  # Set CRS for the single-row GeoDataFrame\n",
    "            print(new_gdf)\n",
    "        else:\n",
    "            print(f\"No features found for {estuary_to_extract}.\")\n",
    "    else:\n",
    "        print(f\"{estuary_to_extract} not found in the dictionary.\")\n",
    "\n",
    "    return new_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47effac6-30af-4735-a3f6-a75896839da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_line(shape): #set indices\n",
    "    \n",
    "    longest_distance = 0\n",
    "    longest_line = None\n",
    "\n",
    "    #extracting the longest line from the shapefile\n",
    "    for index, row in shape.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        exterior_ring = polygon.exterior\n",
    "        #This doesn't exist in Plymouth but no idea about other estuaries\n",
    "        for interior_ring in polygon.interiors:\n",
    "            ring = interior_ring\n",
    "            for i in range(len(ring.coords) - 1):\n",
    "                point1 = ring.coords[i]\n",
    "                point2 = ring.coords[i + 1]\n",
    "                line = LineString([point1, point2])\n",
    "                distance = line.length\n",
    "                #Replace longest distance\n",
    "                if distance > longest_distance:\n",
    "                    longest_distance = distance\n",
    "                    longest_line = line\n",
    "    \n",
    "        for i in range(len(exterior_ring.coords) - 1):\n",
    "            point1 = exterior_ring.coords[i]\n",
    "            point2 = exterior_ring.coords[i + 1]\n",
    "            line = LineString([point1, point2])\n",
    "            distance = line.length\n",
    "            if distance > longest_distance:\n",
    "                longest_distance = distance\n",
    "                longest_line = line\n",
    "                \n",
    "    reprojected_longest_line_gdf = gpd.GeoDataFrame({'geometry': [longest_line]}, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    return reprojected_longest_line_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc183336-984b-44d2-b528-6bb9591f1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_site_number(estuary, input_data, longest_line, est_shape, output_folder_path):\n",
    "    #define the transformer\n",
    "    crs_british = pyproj.CRS('EPSG:27700')\n",
    "    target_crs = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "    #the always_xy argument tells the transformer how to expect the easting/northing variables\n",
    "    transformer = pyproj.Transformer.from_crs(crs_british, target_crs, always_xy=True)\n",
    "\n",
    "    #create copy\n",
    "    full_wims_reproj = input_data.copy()\n",
    "\n",
    "    longitude, latitude = transformer.transform(full_wims_reproj['easting'], full_wims_reproj['northing'])\n",
    "    full_wims_reproj['lat'] = latitude\n",
    "    full_wims_reproj['lon'] = longitude\n",
    "\n",
    "    # Create a GeoDataFrame with list of unique points from lon and lat columns\n",
    "    sites = full_wims_reproj[['lon', 'lat']].drop_duplicates()\n",
    "    \n",
    "    # Create Point geometries using 'lon' and 'lat'\n",
    "    geometry = [Point(xy) for xy in zip(sites['lon'], sites['lat'])]\n",
    "\n",
    "    # Create the GeoDataFrame\n",
    "    sites_gdf = gpd.GeoDataFrame(sites, geometry=geometry)\n",
    "\n",
    "    # Set the CRS of the GeoDataFrame\n",
    "    sites_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    # Reproject the 'sites_gdf' GeoDataFrame to the same CRS as 'reprojected_longest_line_gdf'\n",
    "    sites_reprojected = sites_gdf.to_crs(longest_line.crs)\n",
    "\n",
    "    # Calculate distances to the reprojected longest line\n",
    "    sites_reprojected['DistToEstuary'] = sites_reprojected['geometry'].apply(\n",
    "        lambda x: longest_line[\"geometry\"].distance(x).min()\n",
    "    )\n",
    "\n",
    "    # Sort the GeoDataFrame\n",
    "    sites_reprojected.sort_values('DistToEstuary', inplace=True)\n",
    "    sites_reprojected.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add the 'site_number' column\n",
    "    sites_reprojected['site_number'] = sites_reprojected.index + 1\n",
    "    sites_reprojected['site_number'] = sites_reprojected['site_number'].astype(int)\n",
    "\n",
    "    #Merge on left to link full_wims to sites_reprojected\n",
    "    full_wims_reproj = full_wims_reproj.merge(sites_reprojected, on =['lon','lat'], how = 'left')\n",
    "\n",
    "\n",
    "    fig, ax0 = plt.subplots(nrows=1, ncols=1,figsize=(12, 10))\n",
    "    \n",
    "    est_shape.plot(color='lightgray',ax=ax0)\n",
    "    \n",
    "    scatter = ax0.scatter(full_wims_reproj['lon'], full_wims_reproj['lat'], c=full_wims_reproj['site_number'],\n",
    "                          edgecolor=\"none\", s=70)\n",
    "    longest_line.plot(color='red',ax=ax0)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    ax0.set_title(f'Site map for {estuary}')\n",
    "    ax0.set_xlabel('Easting (m)')\n",
    "    ax0.set_ylabel('Northing (m)')\n",
    "\n",
    "    colorbar = plt.colorbar(scatter)\n",
    "    colorbar.set_label('Site Number')\n",
    "    \n",
    "    output_plot_path = os.path.join(output_folder_path, f'site_map_{estuary}.png')\n",
    "    plt.savefig(output_plot_path, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "    return full_wims_reproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3105087-f69e-4f7a-bdbb-d3f95261769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_times(full_wims_reproj):\n",
    "    full_wims_dt = full_wims_reproj.copy()\n",
    "    full_wims_dt['date']= pd.to_datetime(full_wims_dt['date'], dayfirst=True,format='mixed')\n",
    "    #Get year\n",
    "    full_wims_dt['year'] = full_wims_dt['date'].dt.strftime('%Y')\n",
    "    #Get month\n",
    "    full_wims_dt['month'] = full_wims_dt['date'].dt.strftime('%m')\n",
    "    #Get day\n",
    "    full_wims_dt['day'] = full_wims_dt['date'].dt.strftime('%d')\n",
    "    #Get month/day\n",
    "    full_wims_dt['month_day'] = full_wims_dt['date'].dt.strftime('%m-%d')\n",
    "    #Convert year to number\n",
    "    full_wims_dt['year'] = full_wims_dt['year'].astype('int')\n",
    "    #Convert day to number\n",
    "    full_wims_dt['day'] = full_wims_dt['day'].astype('int')\n",
    "    print(len(full_wims_dt))\n",
    "    print('Dates converted successfully!')\n",
    "    return full_wims_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16fb502-a8af-499e-afa2-d6b0c985bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def screen_sample_depths(estuary,full_wims_dt,output_folder_path):\n",
    "    #Create unique ID to extract sample depth\n",
    "    full_wims_dt['unique_key'] = full_wims_dt['id'].astype(str) + full_wims_dt['sampno'].astype(str) + full_wims_dt['time'].astype(str)\n",
    "    #Select only rows with sample depth info\n",
    "    sample_depth = full_wims_dt[full_wims_dt['detname']=='Sample Depth below surface']\n",
    "    #Remov\n",
    "    sample_numbers_with_non_zero_depth = sample_depth[sample_depth['result'] > 1]['unique_key'].tolist()\n",
    "    \n",
    "    full_wims_surface_samples = full_wims_dt.copy()\n",
    "    full_wims_surface_samples = full_wims_surface_samples[~full_wims_surface_samples['unique_key'].isin(sample_numbers_with_non_zero_depth)]\n",
    "    \n",
    "    sample_depth_after_filter = full_wims_surface_samples[full_wims_surface_samples['detname']=='Sample Depth below surface']\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2,figsize=(18, 10))\n",
    "    \n",
    "    ax0.hist(sample_depth['result'], bins='auto', edgecolor='black', log=True)\n",
    "    ax0.set_title(f'{estuary}: Histogram of sample depths before filtering')\n",
    "    ax0.set_xlabel('Sample depth (m)')\n",
    "    ax0.set_ylabel('Number of samples')\n",
    "\n",
    "    ax1.hist(sample_depth_after_filter['result'], bins='auto', edgecolor='black', log=True)\n",
    "    ax1.set_title(f'{estuary}: Histogram of sample depths after filtering')\n",
    "    ax1.set_xlabel('Sample depth (m)')\n",
    "    ax1.set_ylabel('Number of samples')\n",
    "    \n",
    "    output_plot_path = os.path.join(output_folder_path, f'sample_depth_histogram_{estuary}.png')\n",
    "    plt.savefig(output_plot_path, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "    return full_wims_surface_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea00f1e9-65c4-40fa-ac9b-7adae5b68b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seasons(full_wims_surface_samples):\n",
    "    full_wims_seasons = full_wims_surface_samples.copy()\n",
    "\n",
    "    #found out roughly when the first and last day of each season is - this changes year to year though\n",
    "    full_wims_seasons['astronomic season'] = ''\n",
    "    full_wims_seasons['astronomic season'] = np.where((full_wims_seasons['month_day'] >= '03-30') & (full_wims_seasons['month_day'] <= '06-22'), 'Spring', full_wims_seasons['astronomic season'])\n",
    "    full_wims_seasons['astronomic season'] = np.where((full_wims_seasons['month_day'] > '06-22') & (full_wims_seasons['month_day'] <= '09-23'), 'Summer', full_wims_seasons['astronomic season'])\n",
    "    full_wims_seasons['astronomic season'] = np.where((full_wims_seasons['month_day'] > '09-23') & (full_wims_seasons['month_day'] <'12-22'), 'Autumn', full_wims_seasons['astronomic season'])\n",
    "    full_wims_seasons['astronomic season'].replace('','Winter',inplace=True)\n",
    "    \n",
    "    return full_wims_seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b268a0cd-abb2-4f95-b350-a2e582f46fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salinity calcs\n",
    "def pythagorean(lat1, lon1, lat2, lon2):\n",
    "    return np.sqrt((lat2-lat1)**2 + (lon2-lon1)**2)\n",
    "\n",
    "def closest(data, v):\n",
    "    if not data:\n",
    "        # Handle the case when data is empty (e.g., return a default value or raise an exception)\n",
    "        return None  # Or any other appropriate action\n",
    "\n",
    "    return min(data, key=lambda p: pythagorean(v['lat'], v['lon'], p['lat'], p['lon']))\n",
    "    \n",
    "def assign_average_salinity(estuary,full_wims_seasons,output_folder_path):\n",
    "    wims_sal = full_wims_seasons[full_wims_seasons['detcode'].str.contains('6687|7608|0063')]\n",
    "    #Removal of any brine samples\n",
    "    wims_sal = wims_sal[wims_sal['result'] <= 50] # Removed before salinity grouping to stop outliers changing groupings.\n",
    "    #copy dataframe\n",
    "    wims_sal_group = wims_sal.copy()\n",
    "    #group salinity data by mean for site number\n",
    "    wims_sal_group = wims_sal_group.groupby('site_number')['result'].mean()\n",
    "    #Merge the salinity average value on the grouped salinity\n",
    "    wims_sal_avg = wims_sal.merge(wims_sal_group, on = 'site_number', how='left')\n",
    "    #The results column will be duplicated, hence rename the second one site_avg_salinity\n",
    "    wims_sal_avg.rename(columns={'result_y':'site_avg_salinity'},inplace=True)\n",
    "\n",
    "    #Define the salinity boundary conditions and assign them to a value\n",
    "    conditions = [\n",
    "        wims_sal_avg['site_avg_salinity'] < 0.5,\n",
    "        (wims_sal_avg['site_avg_salinity'] >= 0.5) & (wims_sal_avg['site_avg_salinity'] < 30)\n",
    "    ]\n",
    "\n",
    "    choices = ['freshwater', 'brackish']\n",
    "    wims_sal_avg['salinity_class'] = np.select(conditions, choices, default='saline')\n",
    "    #Remove excess salinity class counts\n",
    "    wims_sal_avg['salinity_class'].value_counts(dropna=False)\n",
    "    #Take the site numbers\n",
    "    unique_sal_values = wims_sal_avg[['site_number','site_avg_salinity','salinity_class']].drop_duplicates()\n",
    "    #Takes a copy of the dataset\n",
    "    full_wims_sal = full_wims_seasons.copy()\n",
    "    #Merge onto the main dataframe to assign any values where there is a dataset\n",
    "    full_wims_sal = pd.merge(full_wims_sal, unique_sal_values, on=['site_number'], how='left')\n",
    "    \n",
    "    #Take a copy of the dataset\n",
    "    full_wims_sal_num_class = full_wims_sal.copy()\n",
    "    full_wims_sal_num_class['salinity_class'].replace({'freshwater':0.0,'brackish':1.0,'saline':2.0},inplace=True)\n",
    "    full_wims_sal_num_class['salinity_class'].value_counts(dropna=False)\n",
    "\n",
    "    #Grab the latitude and longitude for any sample points where there IS NOT a salinity class\n",
    "    missing_salinity_points = full_wims_sal_num_class[full_wims_sal_num_class['salinity_class'].isna()][['lat','lon']].drop_duplicates()\n",
    "    #Grab the latitude and longitude for any sample points where there IS a salinity class\n",
    "    available_salinity_points = full_wims_sal_num_class[full_wims_sal_num_class['salinity_class'].notna()][['lat','lon']].drop_duplicates()\n",
    "\n",
    "    # Create a new DataFrame with closest points\n",
    "    result_data2 = []\n",
    "\n",
    "    #Check to see if there are available salinity points\n",
    "    if not available_salinity_points.empty:\n",
    "        #Check to see if there are any missing salinity points\n",
    "        if not missing_salinity_points.empty:\n",
    "            for index, row in missing_salinity_points.iterrows():\n",
    "                closest_point = closest(available_salinity_points.to_dict(orient='records'), row.to_dict())\n",
    "                if closest_point is not None:\n",
    "                    result_data2.append({\n",
    "                        'missing_sal_lat': row['lat'],\n",
    "                        'missing_sal_lon': row['lon'],\n",
    "                        'closest_lat': closest_point['lat'],\n",
    "                        'closest_lon': closest_point['lon'],\n",
    "                        })\n",
    "                else:\n",
    "                    # Handle the case when there are no closest salinity points (e.g., skip, print a message, or raise an exception)\n",
    "                    print(f\"No closest salinity point found for coordinates {row['lat']}, {row['lon']}\")\n",
    "\n",
    "            # Take the resulting dictionary and convert to DataFrame\n",
    "            result_df2 = pd.DataFrame(result_data2)\n",
    "    \n",
    "            #look in the dataset for where there is salinity measurements\n",
    "            available_salinity_samples = full_wims_sal_num_class[full_wims_sal_num_class['salinity_class'].notna()][['lat','lon','site_avg_salinity','salinity_class','site_number']].drop_duplicates()\n",
    "            available_salinity_samples.rename(columns={'salinity_class':'replaced_salinity'},inplace=True)\n",
    "            available_salinity_samples.rename(columns={'site_avg_salinity':'replaced_site_avg_salinity'},inplace=True)\n",
    "    \n",
    "            #Add the data from available_salinity_samples to the nearest lat_long to each missing sample\n",
    "            filled_nas = pd.merge(result_df2, available_salinity_samples, left_on=['closest_lat','closest_lon'], right_on=['lat','lon'])\n",
    "        \n",
    "            #missing_lat and missing_lon are the coordinates from rows missing salinities in full wims\n",
    "            #we can drop the other columns now because they were just used for the join to grab\n",
    "            #nearest salinity class and site number\n",
    "            filled_nas = filled_nas[['missing_sal_lat','missing_sal_lon','replaced_site_avg_salinity','replaced_salinity','site_number']]\n",
    "    \n",
    "            #Merge the salinity value from filled_nas with the full_wims_sal created above. This will not occur for any samples where salinity already exists\n",
    "            salinity_filled = pd.merge(full_wims_sal_num_class, filled_nas,  left_on=['lat','lon'], right_on=['missing_sal_lat','missing_sal_lon'], how='left')\n",
    "            #Fill the salinity classification\n",
    "            salinity_filled['salinity_class'] = salinity_filled['salinity_class'].fillna(salinity_filled['replaced_salinity'])\n",
    "            salinity_filled['site_avg_salinity'] = salinity_filled['site_avg_salinity'].fillna(salinity_filled['replaced_site_avg_salinity'])\n",
    "            #checking to see that every site has a salinity class\n",
    "            salinity_filled['salinity_class'].value_counts(dropna=False)\n",
    "            #Drop unnecessary columns\n",
    "            salinity_filled.drop(columns=[ 'missing_sal_lon', 'replaced_salinity', 'site_number_y','replaced_site_avg_salinity'],inplace=True)\n",
    "            #Rename the site number\n",
    "            salinity_filled.rename(columns={'site_number_x':'site_number'},inplace=True)\n",
    "            #Copy the dataset\n",
    "            wims_sal_filled = salinity_filled.copy()\n",
    "            #turning classifications back into label/string format because nan values are gone now\n",
    "            wims_sal_filled['salinity_class'].replace({0:'freshwater', 1:'brackish', 2:'saline'},inplace=True)\n",
    "\n",
    "            #Applying a column to highlight if salinity classification was calculated or taken using nearest neighbour\n",
    "            # Replace numeric values with 'Nearest Neighbour'. Checking for not null and a numeric value\n",
    "            wims_sal_filled.loc[wims_sal_filled['missing_sal_lat'].notnull() & wims_sal_filled['missing_sal_lat'].apply(lambda x: isinstance(x, (int, float))), 'missing_sal_lat'] = 'Nearest Neighbour'\n",
    "            # Fill remaining gaps with 'Calculated'\n",
    "            wims_sal_filled['missing_sal_lat'].fillna('Calculated', inplace=True)\n",
    "            # Rename the column\n",
    "            wims_sal_filled_renamed = wims_sal_filled.rename(columns={'missing_sal_lat': 'sal_source'})\n",
    "        \n",
    "        else:\n",
    "            wims_sal_filled_renamed = full_wims_sal_num_class.copy()\n",
    "            print('No missing salinity points to fill') \n",
    "    else:\n",
    "        wims_sal_filled_renamed = full_wims_sal_num_class.copy()\n",
    "        print('No available salinity points to fill with') \n",
    "\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1,figsize=(20, 10))\n",
    "    \n",
    "    shapefile.plot(color='lightgray', ax=ax0)\n",
    "    sns.scatterplot(x='easting', y='northing', palette='husl', hue='salinity_class', data=wims_sal_avg,\n",
    "                                 edgecolor=\"none\", s=70, ax= ax0)\n",
    "    ax0.set_title('Geographical Points Salinity Classifications Before Nearest Neighbour Fill')\n",
    "\n",
    "    shapefile.plot(color='lightgray', ax=ax1)\n",
    "    sns.scatterplot(x='easting', y='northing', palette='husl', hue='salinity_class', data=wims_sal_filled_renamed,\n",
    "                                 edgecolor=\"none\", s=70, ax=ax1)\n",
    "    ax1.set_title('Geographical Points Salinity Classifications After Nearest Neighbour Fill')\n",
    "\n",
    "    output_plot_path = os.path.join(output_folder_path, f'salinity_map_{estuary}.png')\n",
    "    plt.savefig(output_plot_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return wims_sal_filled_renamed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c468fcd6-8dd7-4aa6-a4ff-62f635bf20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seperate_dataframes(wims_sal_filled_renamed):\n",
    "    wims_nd_removed = wims_sal_filled_renamed[wims_sal_filled_renamed['qual'].isna()] #this means there weren't any greater than or less than symbols in these rows\n",
    "    wims_nd_converted = wims_sal_filled_renamed.copy()\n",
    "    wims_nd_converted['result'] = wims_nd_converted.apply(lambda row: row['result'] * 0.5 if row['qual'] == '<' else row['result'], axis=1)\n",
    "    print(len(wims_nd_converted))\n",
    "    print(len(wims_nd_removed))\n",
    "    #Making a column that combines the det codes and labels them either as one of our \n",
    "    #chosen metrics or \"Other\" for filtering purposes\n",
    "    wims_nd_converted['param'] = 'Other'\n",
    "    wims_nd_removed['param'] = 'Other'\n",
    "\n",
    "    for index, value_df in wims_nd_converted['detcode'].items():\n",
    "        for key, value_list in params.items():\n",
    "            if value_df in value_list:\n",
    "                wims_nd_converted.loc[index, 'param'] = key\n",
    "                break\n",
    "\n",
    "    for index, value_df in wims_nd_removed['detcode'].items():\n",
    "        for key, value_list in params.items():\n",
    "            if value_df in value_list:\n",
    "                wims_nd_removed.loc[index, 'param'] = key\n",
    "                break\n",
    "\n",
    "    #need to fix this and make it clean above?\n",
    "    wims_nd_converted.rename(columns={ 'site_number_x':'site_number'},inplace=True)\n",
    "    wims_nd_removed.rename(columns={ 'site_number_x':'site_number'},inplace=True)\n",
    "\n",
    "    return wims_nd_converted, wims_nd_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3365dc7-ff44-4b8b-bcef-6ba2d0aaf898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of detcodes\n",
    "params = {'Temperature': ['0076'], 'pH': ['0061'], 'Dissolved Oxygen':['9924'],'Dissolved Oxygen Saturation':['9901'],\n",
    "          'Suspended Solids at 105C':['0135'], 'Salinity':['1198', '3028', '7608', '7609', '6687'],\n",
    "          'Ammoniacal Nitrogen': ['0111', '9993'], 'Nitrate': ['0117', '9853'], 'Nitrite':['0118', '6485'],\n",
    "          'Biological Oxygen Demand': ['0085', '0088'], 'Copper': ['6450', '6452'], 'Zinc':['3408', '6455'],\n",
    "          'Orthophosphate':['0180', '9856'], 'Alkalinity': ['0162'], 'Turbidity':['6396', '3976']}\n",
    "\n",
    "#dictionary of hexcodes\n",
    "colours = {'Temperature': '#f77189', 'pH': '#dc8932', 'Dissolved Oxygen': '#ae9d31','Dissolved Oxygen Saturation': '#ae9d31',\n",
    "          'Suspended Solids at 105C': '#77ab31', 'Salinity':'#36ada4',\n",
    "          'Ammoniacal Nitrogen': '#4878d0', 'Nitrate': '#cc7af4', 'Nitrite':'#f565cc',\n",
    "          'Biological Oxygen Demand': '#21ada8', 'Copper': '#ff7f00', 'Zinc':'#b2df8a',\n",
    "          'Orthophosphate':'#6a3d9a', 'Alkalinity': '#003399', 'Turbidity':'#b30086'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c36dc3-db4e-44f8-89d4-2019da8b32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_variables(estuary, nd_converted, nd_removed, colours, output_folder_path, filter_title):\n",
    "    for i,j in zip(colours.keys(),colours.values()):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        #scatter plot\n",
    "        sns.scatterplot(x='site_number', y='result', data=nd_converted[nd_converted['param'] == i], color='black', ax=axes[0])\n",
    "        sns.scatterplot(x='site_number', y='result', data=nd_removed[nd_removed['param'] == i], color=j, ax=axes[0])\n",
    "        axes[0].set_title('Scatter Plot')\n",
    "        axes[0].set_xlabel('Site ID')\n",
    "        axes[0].set_ylabel(f'{i}')\n",
    "\n",
    "        #kde plot\n",
    "        sns.kdeplot(x='result', data=nd_converted[nd_converted['param'] == i], fill=True, color='black', ax=axes[1])\n",
    "        sns.kdeplot(x='result', data=nd_removed[nd_removed['param'] == i], fill=True, color=j, ax=axes[1])\n",
    "        axes[1].set_title('KDE Plot')\n",
    "        axes[1].set_xlabel(f'{i}')\n",
    "\n",
    "        plt.suptitle(f'{estuary}: Distribution of {i} Results {filter_title}')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_plot_path = os.path.join(output_folder_path, f'sample_distribution_{i}_{filter_title}_{estuary}.png')\n",
    "        plt.savefig(output_plot_path, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd263c04-c459-412b-80ea-300239338eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(wims_nd_converted, wims_nd_removed, multiple):\n",
    "    #filtering begins\n",
    "    filtered_results_nd_removed = []\n",
    "\n",
    "    chosen_params = wims_nd_removed[wims_nd_removed['param'] != 'Other']['param'].unique() #getting all unique params present in the dataset which\n",
    "    #are not equal to 'Other'\n",
    "\n",
    "    for param_value in chosen_params:\n",
    "        subset = wims_nd_removed[wims_nd_removed['param'] == param_value] #subsetting wims while iterating through the chosen_params\n",
    "    \n",
    "        subset_std = subset['result'].std() * multiple\n",
    "    \n",
    "        subset_mean = subset['result'].mean()\n",
    "    \n",
    "        min_filter = subset_mean - subset_std\n",
    "        max_filter = subset_mean + subset_std\n",
    "    \n",
    "        filtered_subset = subset[(subset['result'] >= min_filter) & (subset['result'] <= max_filter)]\n",
    "    \n",
    "        #add the new filtered subset each time one has been cleaned to the master\n",
    "        filtered_results_nd_removed.append(filtered_subset)\n",
    "\n",
    "    filtered_df_nd_removed = pd.concat(filtered_results_nd_removed, ignore_index=True)\n",
    "\n",
    "    #filtering begins\n",
    "    filtered_results_nd_converted = []\n",
    "\n",
    "    chosen_params = wims_nd_converted[wims_nd_converted['param'] != 'Other']['param'].unique() #getting all unique params present in the dataset which\n",
    "    #are not equal to 'Other'\n",
    "\n",
    "    for param_value in chosen_params:\n",
    "        subset = wims_nd_converted[wims_nd_converted['param'] == param_value] #subsetting wims while iterating through the chosen_params\n",
    "    \n",
    "        subset_std = subset['result'].std() * multiple\n",
    "    \n",
    "        subset_mean = subset['result'].mean()\n",
    "    \n",
    "        min_filter = subset_mean - subset_std\n",
    "        max_filter = subset_mean + subset_std\n",
    "    \n",
    "        filtered_subset = subset[(subset['result'] >= min_filter) & (subset['result'] <= max_filter)]\n",
    "    \n",
    "        #add the new filtered subset each time one has been cleaned to the master\n",
    "        filtered_results_nd_converted.append(filtered_subset)\n",
    "    \n",
    "    filtered_df_nd_converted = pd.concat(filtered_results_nd_converted, ignore_index=True)\n",
    "\n",
    "    return filtered_df_nd_removed, filtered_df_nd_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f41c97c3-3a37-41c6-bf1b-418307a4f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a CSV file with multiple columns\n",
    "csv_file_path = \"C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\estuary_shapefile_dictionary.csv\"\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "data_dict = {}\n",
    "\n",
    "# Read the CSV file and populate the dictionary\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    # Skip the header row if it exists\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    # Iterate through rows and add key-value pairs to the dictionary\n",
    "    for row in csv_reader:\n",
    "        key = row[0]\n",
    "        values = row[1:]  # Collect all values from the second column onwards\n",
    "        data_dict[key] = values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c36680-9ac2-483e-917a-5d9ff762d653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "estuary_list = list(data_dict.keys())\n",
    "print(len(estuary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5d36c4c-a01b-474c-8237-a3fc92cd1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfd_trac = \"C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\WFD_trac_shapefile\\\\WFD_Transitional_and_Coastal_Water_Bodies_Cycle_2.shp\"\n",
    "wfd_trac_shp = gpd.read_file(wfd_trac) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3bee69-9166-47f5-8d9a-86df71c08213",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_sample_points = pd.read_csv(\"C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\Extra_sample_points.csv\")\n",
    "values_to_remove = extra_sample_points['Pointcode'].tolist()\n",
    "#values_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a05d01f-5036-4fd3-9782-3d7fc45a00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_line_df = pd.read_csv(\"C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\Longest_lines_df.csv\")\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "geometry = [LineString([(x1, y1), (x2, y2)]) for x1, y1, x2, y2 in zip(longest_line_df['CoordA_X'], longest_line_df['CoordA_Y'], longest_line_df['CoordB_X'], longest_line_df['CoordB_Y'])]\n",
    "longest_line_list_gdf = gpd.GeoDataFrame(longest_line_df, geometry=geometry)\n",
    "longest_line_list_gdf.crs = 'epsg:27700'\n",
    "# Convert to latitude and longitude (EPSG 4326)\n",
    "longest_line_list_gdf_latlon = longest_line_list_gdf.to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f00d93",
   "metadata": {},
   "source": [
    "# Import all WIMS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c64a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'C:\\Users\\alechutchings\\Documents\\PythonNotebooks\\SeasonalEstuaryChanges\\Dart\\output_plots' already exists.\n",
      "80\n",
      "79\n",
      "No duplicate rows found.\n",
      "            wb_id wb_name  rbd_id    rbd_name        wb_cat    st_area_sh  \\\n",
      "0  GB510804605900    DART     8.0  South West  Transitional  8.319579e+06   \n",
      "\n",
      "     st_perimet                                           geometry  \n",
      "0  63615.157234  POLYGON ((284859.519 56881.427, 284839.000 568...  \n",
      "Dart longest line extracted from shapefile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\3976447997.py:33: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lambda x: longest_line[\"geometry\"].distance(x).min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147318\n",
      "Dates converted successfully!\n",
      "123339\n",
      "89511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\1041832945.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wims_nd_removed['param'] = 'Other'\n",
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\1041832945.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wims_nd_removed.rename(columns={ 'site_number_x':'site_number'},inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dart_process_completed\n",
      "Folder 'C:\\Users\\alechutchings\\Documents\\PythonNotebooks\\SeasonalEstuaryChanges\\Deben\\output_plots' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\1698008481.py:9: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(full_path, dtype={\"INTERPRETATION\": str, \"TEXT_RESULT\": str, \"DETCODE\": str, \"PTCODE\":str}) for full_path in full_paths]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "No duplicate rows found.\n",
      "            wb_id wb_name  rbd_id rbd_name        wb_cat    st_area_sh  \\\n",
      "0  GB520503503900   DEBEN     5.0  Anglian  Transitional  7.819426e+06   \n",
      "\n",
      "     st_perimet                                           geometry  \n",
      "0  68304.521348  POLYGON ((629472.187 250426.187, 629499.026 25...  \n",
      "Deben longest line extracted from table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\3976447997.py:33: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lambda x: longest_line[\"geometry\"].distance(x).min()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13895\n",
      "Dates converted successfully!\n",
      "13839\n",
      "12713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\1041832945.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wims_nd_removed['param'] = 'Other'\n",
      "C:\\Users\\alechutchings\\AppData\\Local\\Temp\\2\\ipykernel_6344\\1041832945.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wims_nd_removed.rename(columns={ 'site_number_x':'site_number'},inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deben_process_completed\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "#reading DETCODE as a string automatically is important as leading zeros get lost if they're read as floats\n",
    "#added the dtypes argument b/c columns have mixed datatypes and its energy intensive for pandas to guess\n",
    "#estuary_list = ['Nene','Nene']\n",
    "\n",
    "#estuary_list = list(data_dict.keys())\n",
    "estuary_list = ['Dart','Deben']\n",
    "\n",
    "# Initialize an empty DataFrame to store coordinates\n",
    "all_longest_line_coords = []\n",
    "\n",
    "for estuary in estuary_list:\n",
    "    try:\n",
    "        #Set up the folder path\n",
    "        folder_path = f\"C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\SeasonalEstuaryChanges\\\\{estuary}\"\n",
    "        #Create an output plot folder if \n",
    "        folder_name = 'output_plots'\n",
    "        output_path = os.path.join(folder_path, folder_name)\n",
    "    \n",
    "        if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "            print(f\"Folder '{output_path}' created.\")\n",
    "        else:\n",
    "            print(f\"Folder '{output_path}' already exists.\")\n",
    "\n",
    "        output_folder_path = os.path.join(folder_path,folder_name)\n",
    "    \n",
    "        #Import stage\n",
    "        folder_path_rawdata = os.path.join(folder_path, 'rawdata')\n",
    "        file_names = [file for file in os.listdir(folder_path_rawdata) if file.endswith('.csv')]\n",
    "        number_files = len(file_names)\n",
    "        full_wims_all = import_csv(number_files, folder_path_rawdata, file_names)\n",
    "\n",
    "        #Remove accidentally added ptcodes\n",
    "        full_wims_all['PTCODE'] = full_wims_all['PTCODE'].astype(str)\n",
    "        full_wims_all['PTCODE'] = full_wims_all['PTCODE'].str.strip()\n",
    "        unique_values_full_wims_all = full_wims_all['PTCODE'].unique()\n",
    "        print(len(unique_values_full_wims_all))\n",
    "        full_wims_all_removed_pointcodes = full_wims_all[~full_wims_all['PTCODE'].isin(values_to_remove)]\n",
    "        unique_values_full_wims_all_removed_pointcodes = full_wims_all_removed_pointcodes['PTCODE'].unique()\n",
    "        print(len(unique_values_full_wims_all_removed_pointcodes))\n",
    "\n",
    "        #duplicate row check\n",
    "        full_wims_no_dup = remove_duplicate_rows(full_wims_all_removed_pointcodes)\n",
    "        \n",
    "        full_wims_no_dup.columns = full_wims_no_dup.columns.str.lower() #converts column names to lower case\n",
    "\n",
    "        #List of accepting purpose codes\n",
    "        accepted_pcs = ['MS',\n",
    "                        'MP',\n",
    "                        'MN',\n",
    "                        'MU']\n",
    "        # Filter the DataFrame based on 'purpose' column\n",
    "        full_wims_no_dup_accepted_purp = full_wims_no_dup[full_wims_no_dup['purpose'].isin(accepted_pcs)]\n",
    "\n",
    "        #folder_path_shapefile = os.path.join(folder_path, 'shapefile')\n",
    "        shapefile = extract_shapefile(data_dict, estuary, wfd_trac_shp, target_crs='EPSG:27700')\n",
    "        #Lat long crs\n",
    "        target_crs = \"EPSG:4326\"\n",
    "    \n",
    "        # Reproject the GeoDataFrame to the target CRS\n",
    "        est_shape = shapefile.to_crs(target_crs)\n",
    "        \n",
    "        if estuary in longest_line_list_gdf_latlon['Estuary'].values:\n",
    "            longest_line = longest_line_list_gdf_latlon[longest_line_list_gdf_latlon['Estuary'] == estuary]\n",
    "            print(f\"{estuary} longest line extracted from table\")\n",
    "        else:\n",
    "            longest_line = find_longest_line(est_shape)\n",
    "            print(f\"{estuary} longest line extracted from shapefile\")\n",
    "\n",
    "        # Extract coordinates of the longest line\n",
    "        longest_line_coords = []\n",
    "        for point in longest_line['geometry'].iloc[0].coords:\n",
    "            lon, lat = point\n",
    "            longest_line_coords.append({'Estuary': estuary, 'Longitude': lon, 'Latitude': lat})\n",
    "        \n",
    "        # Convert the coordinates list to a DataFrame and append to the list\n",
    "        longest_line_coords_df = pd.DataFrame(longest_line_coords)\n",
    "        all_longest_line_coords.append(longest_line_coords_df)\n",
    "\n",
    "            \n",
    "        full_wims_reproj = redefine_site_number(estuary, full_wims_no_dup_accepted_purp, longest_line, est_shape, output_folder_path)\n",
    "        full_wims_dt = convert_date_times(full_wims_reproj)\n",
    "        full_wims_surface_samples = screen_sample_depths(estuary,full_wims_dt,output_folder_path)\n",
    "        full_wims_seasons = get_seasons(full_wims_surface_samples)\n",
    "        wims_sal_filled_renamed = assign_average_salinity(estuary,full_wims_seasons,output_folder_path)\n",
    "        wims_nd_converted, wims_nd_removed = generate_seperate_dataframes(wims_sal_filled_renamed)\n",
    "        plot_all_variables(estuary, wims_nd_converted, wims_nd_removed, colours, output_folder_path, 'before filtering')\n",
    "        filtered_df_nd_removed, filtered_df_nd_converted = filter_data(wims_nd_converted, wims_nd_removed,4)\n",
    "        plot_all_variables(estuary, filtered_df_nd_removed, filtered_df_nd_converted, colours, output_folder_path, 'after filtering')\n",
    "    \n",
    "        output_csv_nd_converted_path = os.path.join(folder_path, f'{estuary}_wims_data_clean_nd_converted.csv')\n",
    "        filtered_df_nd_converted.to_csv(output_csv_nd_converted_path)\n",
    "        output_csv_nd_removed_path = os.path.join(folder_path, f'{estuary}_wims_data_clean_nd_removed.csv')\n",
    "        filtered_df_nd_removed.to_csv(output_csv_nd_removed_path)\n",
    "        print(f'{estuary}_process_completed')\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print the error message\n",
    "        error_message = f\"Error processing {estuary}: {str(e)}\"\n",
    "\n",
    "        # Log the error to a notepad file\n",
    "        log_file_path = os.path.join('C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\SeasonalEstuaryChanges\\\\', 'error_log_cleaning_04_03_2024.txt')\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(error_message + '\\n')\n",
    "        print(error_message)\n",
    "\n",
    "    finally:\n",
    "        # Any cleanup code or additional actions you want to perform regardless of success or failure\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11a973e5-0ae8-413f-a96d-2bcd47606518",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_longest_line_coords_df = pd.concat(all_longest_line_coords, ignore_index=True)\n",
    "all_longest_line_coords_df.to_csv(\"all_longest_line_coords_dart_deben.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4f67f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb932f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
