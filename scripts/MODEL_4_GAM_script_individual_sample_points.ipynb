{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb57a61-8549-4035-9ee2-4b0bffe456c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bac24fd-bdfb-4431-b2cb-0bc0827cfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pygam import LinearGAM, GAM, s, l, te\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import traceback\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import LineString\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07e01e2-1065-4c36-b024-165dc4c348d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv(input_no, *csvs):\n",
    "    if input_no < 1 or input_no > len(csvs):\n",
    "        raise ValueError(\"input_no should be between 1 and the number of CSV files\")\n",
    "    \n",
    "    dfs = [pd.read_csv(csv, dtype={\"ptcode\": str}) for csv in csvs[:input_no]]\n",
    "    full_wims = pd.concat(dfs)\n",
    "    \n",
    "    return full_wims#def import class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0417ff-de97-415e-bd43-3cc43fe79755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pivot(df):\n",
    "    #Create Pivot\n",
    "    Est_pivot = df.pivot_table(values='result',index=['date','time','ptcode','easting','northing','site_avg_salinity','salinity_class'],columns=['detname'])\n",
    "    #Drop the \"Time\" column from results as this duplicates a column elsewhere\n",
    "    Est_pivot = Est_pivot.drop(columns = ['time'],axis=1,errors='ignore')\n",
    "    #Reset the index\n",
    "    Est_pivot.reset_index(inplace=True)\n",
    "    #Create a column for year\n",
    "    Est_pivot['date'] = pd.to_datetime(Est_pivot['date'])\n",
    "    Est_pivot[\"year\"]=Est_pivot[\"date\"].dt.year\n",
    "    #Create a column for month\n",
    "    Est_pivot[\"month\"]=Est_pivot[\"date\"].dt.month\n",
    "    Est_pivot.columns = Est_pivot.columns.str.replace(':', '')\n",
    "    Est_pivot.columns = Est_pivot.columns.str.replace('%', '')\n",
    "    Est_group_all = Est_pivot.copy().reset_index()\n",
    "\n",
    "    return Est_group_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8a7ab0-3838-4826-8f83-6f6c2c797828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_float_year(date):\n",
    "    # Convert the timestamp to a string in the format '%Y-%m-%d'\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Convert the date string to a datetime object\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    \n",
    "    # Extract year, month, and day from the date object\n",
    "    year = date_obj.year\n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "    \n",
    "    # Calculate the fractional part of the year\n",
    "    fractional_year = year + (month - 1) / 12 + (day - 1) / 365\n",
    "    \n",
    "    return fractional_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c57639-f31e-4486-addc-0e5bf699ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_det(Est_group_all,det, minimum_day_range = 3653, minimum_sample_count = 120):\n",
    "\n",
    "    # Group by 'ptcode' and find the earliest and latest date\n",
    "    date_ranges = Est_group_all.groupby('ptcode')['date'].agg(['min', 'max']).reset_index()\n",
    "    # Calculate the difference in days between the earliest and latest dates\n",
    "    date_ranges['day_range'] = (date_ranges['max'] - date_ranges['min']).dt.days\n",
    "    # Filter date ranges where the difference is less than 3653 days (10 years)\n",
    "    date_ranges_filtered = date_ranges[date_ranges['day_range'] > minimum_day_range]\n",
    "    \n",
    "    # Bring together the data only where ptcodes exist in both dataframes\n",
    "    merged_df = pd.merge(Est_group_all, date_ranges_filtered[['ptcode']], on='ptcode', how='inner')\n",
    "\n",
    "    # Group by pintcode and produce states\n",
    "    Est_group = merged_df.groupby(['ptcode'])[det].agg(['mean','std','count']).reset_index()\n",
    "    # Remove any samples where there is not the sufficienct sample count\n",
    "    Est_group = Est_group.loc[Est_group['count'] >= minimum_sample_count]\n",
    "    \n",
    "    # Create a list of point code names which meet the criteria\n",
    "    Est_group_unique_with_dets = Est_group['ptcode'].unique()\n",
    "    \n",
    "    # Grab the coordinates from the est_group_all for later plotting\n",
    "    Filtered_point_code_coords = {code: {'Easting': easting, 'Northing': northing}\n",
    "              for code, easting, northing in zip(Est_group_all['ptcode'], Est_group_all['easting'], Est_group_all['northing'])}\n",
    "\n",
    "    return Est_group_unique_with_dets, Filtered_point_code_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05969c51-820c-40ff-a43d-37fab3e8b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation_from_specific_years(early_period, late_period, samples, XX, num_iterations):\n",
    "\n",
    "    trend_differences = []\n",
    "    \n",
    "    selected_early_data_indices = np.where((XX[:, 2] >= early_period[0]) & (XX[:, 2] <= early_period[1]))[0]\n",
    "    selected_late_data_indices = np.where((XX[:, 2] >= late_period[0]) & (XX[:, 2] <= late_period[1]))[0]\n",
    "\n",
    "    # Check if there are any samples available for the selected years\n",
    "    if len(selected_early_data_indices) == 0 or len(selected_late_data_indices) == 0:\n",
    "        print(\"No samples available for the selected years.\")\n",
    "        return None\n",
    "\n",
    "    for x in range(num_iterations):\n",
    "\n",
    "        idx1 = np.random.choice(selected_early_data_indices)\n",
    "        idx2 = np.random.choice(selected_late_data_indices)\n",
    "        \n",
    "        sample_indices = random.sample(range(len(samples)), 2)\n",
    "        selected_early_samples = samples[sample_indices[0],idx1]\n",
    "        selected_late_samples = samples[sample_indices[1],idx2]\n",
    "\n",
    "        # Calculate difference\n",
    "        difference = (selected_late_samples - selected_early_samples)/(XX[idx2, 2] - XX[idx1,2])\n",
    "        # Append to a dataframe\n",
    "        trend_differences.append(difference)\n",
    "\n",
    "    return trend_differences      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eab1aff-881e-4c5c-82ec-24e5946e9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to allow monte carlo simulations\n",
    "def perform_monte_carlo_simulation(samples, XX_years, num_iterations, year_index):\n",
    "\n",
    "    if XX_years[:,2].max() - XX_years[:,2].min() <= year_index:\n",
    "        print(f\"The range of years is not sufficient for analysis (less than or equal to {year_index} years).\")\n",
    "        return None\n",
    "        \n",
    "    trend_differences = []\n",
    "\n",
    "    #Run through iteration\n",
    "    for x in range(num_iterations):\n",
    "        # Maximum number of attempts to find an appropriate pair of time segements to sample from\n",
    "        max_attempts = 1000\n",
    "        # Take a couple of random indices from the yearly dependence to sample from the normal distribution\n",
    "        sample_indices = random.sample(range(len(samples)), 2)\n",
    "        # Attempt to find a pair of numbers that are greater than 10 years apart\n",
    "        for _ in range(max_attempts):\n",
    "            # Allow the first yearly indices to be any number from 0 to n-1 (to allow for idx2)\n",
    "            idx1 = np.random.randint(0, len(XX_years[:,2])-1)\n",
    "            # Ensure idx2 is greater than idx1 (so the yearly change is in the right direction!)\n",
    "            idx2 = np.random.randint(idx1 + 1, len(XX_years[:,2]))  # Ensuring idx2 is greater than idx1\n",
    "            # Check the difference in years exceeds the year index\n",
    "            if XX_years[:,2][idx2] - XX_years[:,2][idx1] >= year_index:\n",
    "            #Take the random sample indices and the yearly indices for early and late samples\n",
    "                sample_early = samples[sample_indices[0], idx1]\n",
    "                sample_late = samples[sample_indices[1], idx2]\n",
    "                # Calculate difference\n",
    "                difference = (sample_late - sample_early)/(XX_years[idx2, 2] - XX_years[idx1,2])\n",
    "                # Append to a dataframe\n",
    "                trend_differences.append(difference)\n",
    "                # Break the loop once the conditions are met\n",
    "                break\n",
    "\n",
    "    return trend_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7420a7ac-cb37-41c6-8a44-8bb03d07855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, there are numerous terms in this which need changing if the set of independent variables are altered.\n",
    "\n",
    "def run_GAM(Est_group_unique_with_dets, Est_group_all, det, GAM_output_folder_path):\n",
    "    \n",
    "    #Define the pointcode list\n",
    "    pointcode_list = Est_group_unique_with_dets\n",
    "\n",
    "    #Set blank DF\n",
    "    trend_result_list = []\n",
    "\n",
    "    for pointcode in pointcode_list:\n",
    "        # Assuming 'Est_group_all' is your original DataFrame\n",
    "        Test_pt = Est_group_all.loc[Est_group_all['ptcode'] == pointcode].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "        # Date tp datetime\n",
    "        Test_pt.loc[:, 'date'] = pd.to_datetime(Test_pt['date'])\n",
    "        # Convert date to day of year\n",
    "        Test_pt.loc[:, 'day_of_year'] = Test_pt['date'].dt.dayofyear\n",
    "\n",
    "        # Apply the float_date function from above\n",
    "        Test_pt['float_date'] = Test_pt['date'].apply(date_to_float_year)\n",
    "\n",
    "        # This section converts time to a continuous scale\n",
    "        # Extract hours and minutes\n",
    "        hour = Test_pt['time'] // 100\n",
    "        minutes = Test_pt['time'] % 100\n",
    "        # Convert to continuous numeric format (minutes since midnight)\n",
    "        Test_pt.loc[:, 'time_converted'] = hour * 60 + minutes\n",
    "\n",
    "        # Handle NaN values if 'time' column contains them -CHECK THIS\n",
    "        Test_pt['time_converted'] = Test_pt['time_converted'].fillna(-1)  # Replace NaN with a suitable value\n",
    "\n",
    "        #Create independent variables\n",
    "        X = Test_pt[['time_converted', 'day_of_year','float_date']].values\n",
    "        #Create dependent variables\n",
    "        Y = Test_pt[det].values\n",
    "\n",
    "        # Remove rows with NaN values in Y. Note X gets removed in the same way.\n",
    "        mask = ~np.isnan(Y)\n",
    "        X = X[mask]\n",
    "        Y = Y[mask]\n",
    "\n",
    "        #Failsafe to break loop in case there is only one year in the data\n",
    "        if len(np.unique(X[:, 2])) <= 1:\n",
    "            print(f\"Skipping analysis for pointcode {pointcode} as there is only one unique year.\")\n",
    "            continue  # Skip to the next iteration of the loop\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Create and train the GAM model\n",
    "        spline_no=5\n",
    "        # Create and train the GAM model\n",
    "        gam = GAM(s(0, n_splines=spline_no) + s(1,n_splines=spline_no,basis='cp') + s(2,n_splines=spline_no)).gridsearch(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = gam.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        #======================PLOTTING BEGINS============================\n",
    "\n",
    "        # Plotting actual vs predicted values\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        # Sticking a 1:1 line in\n",
    "        plt.plot([min(y_test)-1, max(y_test)+1], [min(y_test)-1, max(y_test)+1], '--', color='red', label='1:1 Line')\n",
    "        plt.title(f'Actual vs Predicted Values - {pointcode}')\n",
    "        plt.xlabel(f'Actual {det}')\n",
    "        plt.ylabel(f'Predicted {det}')\n",
    "        #Exporting figure\n",
    "        Actual_predicted_path = os.path.join(GAM_output_folder_path, f'ActualVSPredicted_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Actual_predicted_path )\n",
    "        plt.clf()\n",
    "\n",
    "        # Plotting residuals\n",
    "        residuals = y_test - y_pred\n",
    "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "        plt.title(f'Residuals Plot - {pointcode}')\n",
    "        plt.xlabel(f'Predicted {det}')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "        #Exporting figure\n",
    "        Residuals_path = os.path.join(GAM_output_folder_path, f'Residuals_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Residuals_path)\n",
    "        plt.clf()\n",
    "\n",
    "        #Plot the partial dependencies\n",
    "        feature_names = ['time', 'day_of_year','float_date']\n",
    "\n",
    "        fig, axs = plt.subplots(1, len(feature_names), figsize=(16, 4))\n",
    "        for i, ax in enumerate(axs):\n",
    "            XX = gam.generate_X_grid(term=i)\n",
    "            pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n",
    "            ax.plot(XX[:, i], pdep)\n",
    "            ax.plot(XX[:, i], confi, c='r', ls='--')\n",
    "            ax.set_title(f'Partial dependence of {feature_names[i]}', fontsize = 10)\n",
    "        plt.tight_layout()\n",
    "        #Exporting figure\n",
    "        Dependencies_path = os.path.join(GAM_output_folder_path, f'Dependencies_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Dependencies_path)\n",
    "        plt.clf()\n",
    "\n",
    "        # This line extracts the partial dependence for the 'year' feature. There's probably a better way to combine with above code.\n",
    "        XX = gam.generate_X_grid(term=2)\n",
    "        pdep, confi = gam.partial_dependence(term=2, X=XX, width=0.95)\n",
    "\n",
    "        # Plot the partial dependence for 'year'\n",
    "        plt.plot(XX[:, 2], pdep)\n",
    "        plt.fill_between(XX[:, 2], confi[:, 0], confi[:, 1], alpha=0.2, color='r')\n",
    "        plt.title('Partial Dependence Plot for Year')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Partial Dependence')\n",
    "        #Exporting figure\n",
    "        Yearly_dependencies_path = os.path.join(GAM_output_folder_path, f'Yearly_dependencies_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Yearly_dependencies_path)\n",
    "        plt.close('all')\n",
    "        \n",
    "        #=======================\n",
    "        \n",
    "        # Take random samples from the dataset (X,Y) to show the uncertainty on the predictions\n",
    "        for response in gam.sample(X, Y, quantity='y', n_draws=100, sample_at_X=XX):\n",
    "            plt.scatter(XX[:, 2], response, alpha=0.03, color='k')  # Assuming XX[:, 0] is the appropriate feature for X-axis\n",
    "        #print(response)\n",
    "        plt.plot(XX[:, 2], gam.predict(XX), 'r--')  # Assuming XX[:, 0] is the appropriate feature for X-axis\n",
    "        plt.title('Draw samples from the posterior of the coefficients')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Predicted temperature')\n",
    "        Random_model_sample_dependencies_path = os.path.join(GAM_output_folder_path, f'Random_model_posterior_sample_distribution_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Random_model_sample_dependencies_path)\n",
    "        plt.close('all')\n",
    "\n",
    "        # Extracting mean and confidence intervals from pdep and confi\n",
    "        mean_year_dependence = pdep\n",
    "        lower_bound = confi[:, 0]\n",
    "        upper_bound = confi[:, 1]\n",
    "\n",
    "        # Calculating standard deviation based on confidence intervals\n",
    "        std_dev = (upper_bound - lower_bound) / (2 * 1.96)  # Assuming 1.96 corresponds to 95% confidence interval\n",
    "\n",
    "        # Generating samples based on the partial dependence of 'year'\n",
    "        num_samples = 100  # The number of samples you want to generate\n",
    "        # Randomly sample assuming a normal distribution of variance\n",
    "        samples = np.random.normal(mean_year_dependence, std_dev, size=(num_samples, len(mean_year_dependence)))\n",
    "\n",
    "        # Plotting samples\n",
    "        for sample in samples:\n",
    "            plt.plot(XX[:, 2], sample, alpha=0.03, color='k')\n",
    "        plt.plot(XX[:, 2], pdep, 'r--')  # Assuming XX[:, 3] is the appropriate feature for X-axis\n",
    "        plt.title('Samples from the posterior of the coefficients')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Response')\n",
    "        Random_yearly_dependency_sample_path = os.path.join(GAM_output_folder_path, f'Random_yearly_dependence_posterior_sample_distribution_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Random_yearly_dependency_sample_path)\n",
    "        plt.close('all')\n",
    "\n",
    "        # Define the total number of iterations to run through monte carlo\n",
    "        num_iterations = 10000\n",
    "\n",
    "        # Set up blank results table\n",
    "        trend_differences_year = []\n",
    "\n",
    "        # =======================================================\n",
    "        # HERE IS THE OPTIONS FOR HOW TO RUN THE TREND ANALYSIS - Comment one out\n",
    "        # ======================================================\n",
    "\n",
    "        # =/=/=/=/=/=/=/=/=/=/=/=/=/=//=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=\n",
    "        # OPTION 1 = Specify two yearly periods\n",
    "\n",
    "        #early_period = [2005.5,2006.5]\n",
    "        #later_period=[2021.5,2022.5]\n",
    "    \n",
    "        #trend_differences_year = monte_carlo_simulation_from_specific_years(early_period, later_period, samples, XX, num_iterations)\n",
    "        # =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=\n",
    "    \n",
    "        # =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=\n",
    "        # OPTION 2 = Specifya yearly interval\n",
    "    \n",
    "        year_index = 10\n",
    "\n",
    "        trend_differences_year = perform_monte_carlo_simulation(samples, XX, num_iterations, year_index)\n",
    "        # =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=\n",
    "\n",
    "        # If trend_differences_year is None, continue to the next iteration\n",
    "        if trend_differences_year is None:\n",
    "            print(f\"Skipping analysis for pointcode {pointcode} as monte_carlo_simulation_from_specific_years returned None.\")\n",
    "            continue\n",
    "\n",
    "        # Plot histogram of trend differences to check the monte carlo simulation\n",
    "        plt.hist(trend_differences_year, bins=50, color='red', edgecolor='black')\n",
    "        plt.title('Histogram of Trend Differences')\n",
    "        plt.xlabel('Difference in Trend')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        Monte_carlo_histo_sample_path = os.path.join(GAM_output_folder_path, f'Monte_carlo_histogram_{pointcode}_{estuary}.png')\n",
    "        plt.savefig(Monte_carlo_histo_sample_path)\n",
    "        plt.close('all')\n",
    "    \n",
    "        # Calculate statistics of trend differences\n",
    "        mean_difference = np.mean(trend_differences_year)\n",
    "        std_dev_difference = np.std(trend_differences_year)\n",
    "    \n",
    "        # Print or use these statistics as needed\n",
    "        print(\"Mean difference in trend:\", mean_difference)\n",
    "        print(\"Standard deviation of difference in trend:\", std_dev_difference)\n",
    "\n",
    "        #=============================Print to CSV============================================\n",
    "        #Appending to a dictionaries for a csv\n",
    "        trend_result_list.append({\n",
    "            'pointcode': pointcode,\n",
    "            'Average': mean_difference,\n",
    "            'Std': std_dev_difference,\n",
    "            'r_squared': r2,\n",
    "            'Mean Squared Error': mse\n",
    "        })\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = pd.DataFrame(trend_result_list)\n",
    "        # Export the DataFrame to a CSV file\n",
    "        csv_path = os.path.join(GAM_output_folder_path, f'Model_4_output_{estuary}_trend_result_list.csv')\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        #=============================Print to Dict============================================\n",
    "        # Append the information to the corresponding dictionary in Filtered_point_code_coords\n",
    "        Filtered_point_code_coords[pointcode]['Average'] = mean_difference\n",
    "        Filtered_point_code_coords[pointcode]['Std'] = std_dev_difference\n",
    "        Filtered_point_code_coords[pointcode]['r_squared'] = r2\n",
    "        Filtered_point_code_coords[pointcode]['Mean Squared Error'] = mse\n",
    "\n",
    "        print(f'{pointcode} completed successfully')\n",
    "    \n",
    "    print('Run completed successfully')\n",
    "\n",
    "    return trend_result_list, Filtered_point_code_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84ca307-9edd-48d8-851a-e32714907ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract the estuary from the wider WFD TraC and coastal waterbody shapefile\n",
    "def extract_shapefile(data_dict, estuary_to_extract, input_shapefile, target_crs='EPSG:xxxx'):\n",
    "    if estuary_to_extract in data_dict:\n",
    "        # Get the corresponding values from the dictionary\n",
    "        corresponding_values = data_dict[estuary_to_extract]\n",
    "\n",
    "        # Initialize an empty GeoDataFrame\n",
    "        combined_gdf = gpd.GeoDataFrame()\n",
    "    \n",
    "        # Iterate through each corresponding value\n",
    "        for value in corresponding_values:\n",
    "            # Filter rows based on the current value\n",
    "            subset_gdf = input_shapefile[input_shapefile['wb_name'] == value]\n",
    "\n",
    "            # Check if the subset is not empty before combining\n",
    "            if not subset_gdf.empty:\n",
    "                # Concatenate the subset to the combined GeoDataFrame\n",
    "                combined_gdf = pd.concat([combined_gdf, subset_gdf], ignore_index=True)\n",
    "\n",
    "        # Check if there is more than one row for the key\n",
    "        if len(combined_gdf) > 1:\n",
    "            # Use unary_union to combine geometries\n",
    "            combined_geometry = unary_union(combined_gdf['geometry'])\n",
    "\n",
    "            # Create a new GeoDataFrame with the combined geometry\n",
    "            new_gdf = gpd.GeoDataFrame(geometry=[combined_geometry], crs=target_crs)\n",
    "\n",
    "            # Print or do further processing with the new GeoDataFrame\n",
    "            print(new_gdf)\n",
    "        elif not combined_gdf.empty:\n",
    "            # Only one row, no need to combine, use the original GeoDataFrame\n",
    "            new_gdf = combined_gdf.copy()\n",
    "            new_gdf.crs = target_crs  # Set CRS for the single-row GeoDataFrame\n",
    "            print(new_gdf)\n",
    "        else:\n",
    "            print(f\"No features found for {estuary_to_extract}.\")\n",
    "    else:\n",
    "        print(f\"{estuary_to_extract} not found in the dictionary.\")\n",
    "\n",
    "    return new_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a39f3038-4f23-4953-a3bc-97f5d770d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\GAM_models_temp_do\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "print(parent_directory)\n",
    "input_model_files_directory = os.path.join(parent_directory, \"input_model_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e946682c-84cd-4695-92ed-1edba2ebcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_directory = os.path.join(parent_directory, \"wfd_shapefile\")\n",
    "wfd_trac_file_path = os.path.join(shapefile_directory, \"WFD_Transitional_and_Coastal_Water_Bodies_Cycle_2.shp\")\n",
    "wfd_trac_shp = gpd.read_file(wfd_trac_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e401c591-fac9-4e9e-ad62-21cd6754b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(input_model_files_directory, \"estuary_shapefile_dictionary.csv\")\n",
    "#csv_file_path = \"C:\\\\Users\\\\alechutchings\\\\Documents\\\\PythonNotebooks\\\\estuary_shapefile_dictionary.csv\"\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "data_dict = {}\n",
    "\n",
    "# Read the CSV file and populate the dictionary\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    # Skip the header row if it exists\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    # Iterate through rows and add key-value pairs to the dictionary\n",
    "    for row in csv_reader:\n",
    "        key = row[0]\n",
    "        values = row[1:]  # Collect all values from the second column onwards\n",
    "        data_dict[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb67d527-2837-4144-b3ef-8167259cb385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "# Create the estuary list\n",
    "estuary_list = list(data_dict.keys())\n",
    "print(len(estuary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0837fa7-a176-4e7e-a4ab-ef0a9a6156de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'C:\\Users\\alechutchings\\GAM_models_temp_do\\output_data\\Poole' already exists.\n",
      "Folder 'C:\\Users\\alechutchings\\GAM_models_temp_do\\output_data\\Poole\\MODEL_4_GAM_outputs_individual_points_Temperature of Water_Yearly_10_interval' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 45% (5 of 11) |###########              | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.06153843170660032\n",
      "Standard deviation of difference in trend: 0.01566425412169859\n",
      "50590110 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 36% (4 of 11) |#########                | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.007436225870397081\n",
      "Standard deviation of difference in trend: 0.05664249807468234\n",
      "50900149 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 45% (5 of 11) |###########              | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.032779523877835934\n",
      "Standard deviation of difference in trend: 0.0463742383617279\n",
      "50900387 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 36% (4 of 11) |#########                | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.028629657635705446\n",
      "Standard deviation of difference in trend: 0.057203029304605496\n",
      "50950106 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 36% (4 of 11) |#########                | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.016475679112228275\n",
      "Standard deviation of difference in trend: 0.054168627908217734\n",
      "50950125 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 36% (4 of 11) |#########                | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.017715970935274026\n",
      "Standard deviation of difference in trend: 0.05282712223051384\n",
      "50950217 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 54% (6 of 11) |#############            | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.01858463236615134\n",
      "Standard deviation of difference in trend: 0.05206244350689448\n",
      "50950249 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      " 36% (4 of 11) |#########                | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.044436070140273225\n",
      "Standard deviation of difference in trend: 0.05744211742712595\n",
      "50950270 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in trend: 0.07920357619776051\n",
      "Standard deviation of difference in trend: 0.06359879911531768\n",
      "50950300 completed successfully\n",
      "Run completed successfully\n",
      "            wb_id        wb_name  rbd_id    rbd_name        wb_cat  \\\n",
      "0  GB520804415800  POOLE HARBOUR     8.0  South West  Transitional   \n",
      "\n",
      "     st_area_sh     st_perimet  \\\n",
      "0  3.309813e+07  157330.942598   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((404852.115 88281.046, 404839.000 882...  \n",
      "Average: 0.03408886309358069\n",
      "Standard Deviation: 0.02360200794313415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alechutchings\\AppData\\Local\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\geopandas\\plotting.py:313: UserWarning: You passed a edgecolor/edgecolors ('grey') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  collection = ax.scatter(x, y, vmin=vmin, vmax=vmax, cmap=cmap, **kwargs)\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "estuary_list = ['Poole']\n",
    "\n",
    "#det = 'Oxygen, Dissolved as O2'\n",
    "det = 'Temperature of Water'\n",
    "#det = 'Oxygen, Dissolved,  Saturation'\n",
    "\n",
    "#trend_sample_option = '1990_2022'\n",
    "#trend_sample_option = '1990_2006'\n",
    "#trend_sample_option = '2006_2022'\n",
    "trend_sample_option = 'Yearly_10_interval'\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "for estuary in estuary_list:\n",
    "    try:\n",
    "        output_data_directory = os.path.join(parent_directory, \"output_data\")\n",
    "        estuary_folder_path = os.path.join(output_data_directory,f'{estuary}')\n",
    "\n",
    "        if not os.path.exists(estuary_folder_path):\n",
    "            os.mkdir(estuary_folder_path)\n",
    "            print(f\"Folder '{estuary_folder_path}' created.\")\n",
    "        else:\n",
    "            print(f\"Folder '{estuary_folder_path}' already exists.\")\n",
    "            \n",
    "        # Create an output plot folder for the GAM models\n",
    "        folder_name = f'MODEL_4_GAM_outputs_individual_points_{det}_{trend_sample_option}'\n",
    "        # Specify the full folder output\n",
    "        GAM_output_path = os.path.join(estuary_folder_path, folder_name)\n",
    "\n",
    "        # Set up a folder if it does not already exist\n",
    "        if not os.path.exists(GAM_output_path):\n",
    "            os.mkdir(GAM_output_path)\n",
    "            print(f\"Folder '{GAM_output_path}' created.\")\n",
    "        else:\n",
    "            print(f\"Folder '{GAM_output_path}' already exists.\")\n",
    "\n",
    "        # Specify the path for the csv improt\n",
    "        csv_input_path = os.path.join(estuary_folder_path, f'{estuary}_wims_data_clean_nd_removed.csv')\n",
    "        # Import the csv file\n",
    "        df = import_csv(1,csv_input_path)\n",
    "        #This is to ensure any pointcodes are kept constant (some have duplicates with blank spaces)\n",
    "        df['ptcode'] = df['ptcode'].astype(str).str.strip()\n",
    "        # Set up the pivot table\n",
    "        Est_group_all = create_pivot(df)\n",
    "        # Set up the filtering and pointcode list\n",
    "        Est_group_unique_with_dets, Filtered_point_code_coords = filter_by_det(Est_group_all,det)\n",
    "        # Run the GAM model code\n",
    "        trend_result_list, Filtered_point_code_coords = run_GAM(Est_group_unique_with_dets, Est_group_all, det, GAM_output_path)\n",
    "\n",
    "        # Extract the estuary shapefile\n",
    "        est_shape = extract_shapefile(data_dict, estuary, wfd_trac_shp, target_crs='EPSG:27700')\n",
    "\n",
    "        # Set up the geodataframe from the pointcodes specified in the GAM model\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            data=[{\n",
    "                'pointcode': key,\n",
    "                'Easting': value['Easting'],\n",
    "                'Northing': value['Northing'],\n",
    "                'geometry': Point(value['Easting'], value['Northing']),\n",
    "                **value  # Add additional properties like Average, Std, etc.\n",
    "            } for key, value in Filtered_point_code_coords.items()],\n",
    "            geometry='geometry'  # Specify the geometry column\n",
    "        )\n",
    "        \n",
    "        # Set the results list from the gam model to a dataframe \n",
    "        trend_result_list_df = pd.DataFrame(trend_result_list)\n",
    "    \n",
    "        # Calculate average (mean)\n",
    "        average_value = trend_result_list_df['Average'].mean()\n",
    "\n",
    "        # Calculate standard deviation\n",
    "        std_deviation_value = trend_result_list_df['Average'].std()\n",
    "\n",
    "        # Print or use the calculated values\n",
    "        print(f'Average: {average_value}')\n",
    "        print(f'Standard Deviation: {std_deviation_value}')\n",
    "\n",
    "        # ============================ PLOTTING BEGINS FOR MAP ==============================================\n",
    "        fig, ax0 = plt.subplots(nrows=1, ncols=1,figsize=(12, 10))\n",
    "        # Plot the shapefile\n",
    "        est_shape.plot(color='lightblue', ax=ax0)\n",
    "        # Plot points with NaN values separately (if any)\n",
    "        nan_color = 'gray'  # Choose a color for NaN values\n",
    "        # Set up a cross if samples did not meet the criteria\n",
    "        gdf[gdf['Average'].isna()].plot(ax=ax0, marker='x', markersize=75, edgecolor='grey', linewidth=0.5, legend_kwds={'label': \"NaN\"})\n",
    "\n",
    "        # Plot the average values on the map\n",
    "        gdf.plot(column='Average', cmap='coolwarm', vmin=-0.2, vmax=0.2, legend=True, ax=ax0, marker='o', markersize=75, edgecolor='black', linewidth=0.5, legend_kwds={'label': \"Temperature increase per year derived from GAM (degrees celsius/yr)\"})\n",
    "\n",
    "        # Add a legend\n",
    "        ax0.set_title(f'Distribution of trends across {estuary}')\n",
    "        ax0.set_xlabel('Easting (m)')\n",
    "        ax0.set_ylabel('Northing (m)')\n",
    "        ax0.legend()\n",
    "        map_path = os.path.join(GAM_output_path, f'{estuary}_GAM_map_{det}_{trend_sample_option}.png')\n",
    "        plt.savefig(map_path)\n",
    "        plt.close('all')\n",
    "        # ====================== PLOTTING ENDS =================================================================\n",
    "    \n",
    "        # Add a column with the name of the estuary\n",
    "        trend_result_list_df['Estuary'] = estuary\n",
    "    \n",
    "        # Append the DataFrame to a master DataFrame (if it already exists)\n",
    "        if 'master_df' not in locals():\n",
    "            master_df = trend_result_list_df.copy()\n",
    "        else:\n",
    "            master_df = pd.concat([master_df, trend_result_list_df], ignore_index=True)\n",
    "    \n",
    "    #Create an error log for issues but permit the loop to continue\n",
    "    except Exception as e:\n",
    "        # Print the error message\n",
    "        error_message = f\"Error processing {estuary}: {str(e)}\"\n",
    "\n",
    "        # Log the error to a notepad file\n",
    "        log_file_path = os.path.join(output_data_directory,f'error_log_GAM_Model4_{det}_{trend_sample_option}.txt')\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(error_message + '\\n')\n",
    "        print(error_message)\n",
    "\n",
    "    finally:\n",
    "        # Any cleanup code or additional actions you want to perform regardless of success or failure\n",
    "        pass\n",
    "\n",
    "# Now, master_df contains all the data with an additional 'Estuary' column. Save to csv for output\n",
    "master_folder_path = os.path.join(output_data_directory,f\"Model_4_output_file_{det}_{trend_sample_option}.csv\")\n",
    "master_df.to_csv(master_folder_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1217bb-8c6c-4f48-b17a-eba8041a8e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af917e-7f86-49df-abf8-efaa07532eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
